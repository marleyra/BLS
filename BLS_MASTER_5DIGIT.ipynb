{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f9a704e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9e62b2",
   "metadata": {},
   "source": [
    "### 1. Load BLS raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f49fc16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = ['1990', '2000', '2010', '2020']\n",
    "files = []\n",
    "\n",
    "# create folder of all raw files\n",
    "for year in years:\n",
    "    folder = sorted(os.listdir('../BLS_raw/' + year + '.annual.by_industry'))\n",
    "    # create list of 5 digit NAICS file names\n",
    "    for file in folder[1:]:\n",
    "        if len(file.split(' ')[1]) == 5:\n",
    "            files.append(file)\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb82864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load naics codes needed for manuscript\n",
    "k5 = pd.read_csv('../kurt_5digit_codes.csv')\n",
    "k5 = k5['NAICS_code'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36dc6f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retain only industries of interest\n",
    "codes = k5\n",
    "files2 = []\n",
    "\n",
    "for file in files:\n",
    "    for code in codes:\n",
    "        if code in file[12:17]:\n",
    "            files2.append(file)\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "262d05c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_list = []\n",
    " \n",
    "# loop through year and file to append dataframes into a list\n",
    "for year in years:\n",
    "    for file in files2:\n",
    "        if year in file:\n",
    "            temp_df = pd.read_csv('../BLS_raw/' + year + '.annual.by_industry/' + file)\n",
    "            mask_non50 = ((temp_df['area_fips'].str[-3:] == '000') | \n",
    "                          (temp_df['area_fips'].str[:1] == 'C') | \n",
    "                          (temp_df['area_fips'].str[:2] == 'US') | \n",
    "                          (temp_df['area_fips'].str[:1] == '7'))\n",
    "            temp_df = temp_df[~mask_non50] # apply Kurt's filter within loop\n",
    "            \n",
    "            # add industry column name to df\n",
    "            if file[:4] == '2020':\n",
    "                temp_df['name'] = file[30:-4]\n",
    "            else:\n",
    "                temp_df['name'] = file[18:-4]\n",
    "            \n",
    "            df_list.append(temp_df)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "# store dataframes in dictionary\n",
    "names = []\n",
    "for x in range(0, len(files2)):\n",
    "    names.append('df' + files2[x][12:17]+ '_' + files2[x][:4])\n",
    "\n",
    "d = dict(zip(names, df_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d59a23",
   "metadata": {},
   "source": [
    "### 2. Process raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b01fe52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop undefined counties\n",
    "for key in d.keys():\n",
    "    d[key] = d[key][d[key]['area_fips'].str[-3:] != '999']\n",
    "    #print(d[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c42af806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine public and private sector employment within-county\n",
    "for key in d.keys():\n",
    "    d[key] = (d[key].groupby(by=['name', 'area_fips', 'area_title'], as_index=False)['annual_avg_emplvl'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a62ae538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load typology (US Census geographical classifications) and regional data\n",
    "reg = pd.read_csv('typology/regions.csv')\n",
    "typ = pd.read_csv('typology/typology.csv')\n",
    "\n",
    "# change county codes to proper fips\n",
    "typ['fips'] = typ['fips'].astype(str).str.zfill(5)\n",
    "\n",
    "# merge BLS data with '03, '13, & '20' geographical classifications, retaining all counties from typ file\n",
    "for key in d.keys():\n",
    "    d[key] = d[key].merge(typ, how='left', # no undefined counties in typ, so can do outer merge to preserve\n",
    "                          left_on='area_fips', right_on='fips')\n",
    "\n",
    "# assign regions to BLS data\n",
    "for key in d.keys():\n",
    "    for k, v in dict(zip(reg.STATE, reg.REGION)).items():\n",
    "        d[key].loc[d[key]['State'] == k, ['region']] = v\n",
    "    d[key].loc[d[key]['area_title'] == 'District of Columbia', 'region'] = 'Mid-Atlantic' # give DC a region\n",
    "    \n",
    "# export selected columns\n",
    "#final_cols = ['name', 'fips', 'area_fips', 'County Title', 'State', 'region',\n",
    "#              'type_census03', 'type_bls13', 'type_census20', 'type_kurt20',\n",
    "#              'annual_avg_emplvl']\n",
    "\n",
    "final_cols = ['name', 'fips', 'County Title', 'State', 'region',\n",
    "              'type_kurt20', 'annual_avg_emplvl']\n",
    "\n",
    "for key in d.keys():\n",
    "    d[key][final_cols].to_csv('my_naics5/naics_' + key[2:] + '.csv', index_label=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd65d814",
   "metadata": {},
   "source": [
    "### 3. Calculate employment change columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f9ffb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load new NAICS CSVs\n",
    "folder = sorted(os.listdir('my_naics5'))\n",
    "uniquecodes = pd.Series(folder[1:]).str[6:11].unique()\n",
    "path = 'my_naics5/naics_'\n",
    "\n",
    "# use big loop to update all previously processed NAICS files\n",
    "\n",
    "# capture the incomplete sets\n",
    "missing_years = []\n",
    "weird_years = []\n",
    "\n",
    "for code in uniquecodes:\n",
    "    try:\n",
    "        df90 = pd.read_csv(path + code + '_1990.csv')\n",
    "    except:\n",
    "        missing_years.append([code, '1990'])\n",
    "    try:\n",
    "        df00 = pd.read_csv(path + code + '_2000.csv')\n",
    "    except:\n",
    "        missing_years.append([code, '2000'])\n",
    "    try:    \n",
    "        df10 = pd.read_csv(path + code + '_2010.csv')\n",
    "    except:\n",
    "        missing_years.append([code, '2010'])\n",
    "    try:    \n",
    "        df20 = pd.read_csv(path + code + '_2020.csv')\n",
    "    except:\n",
    "        missing_years.append([code, '2020'])\n",
    "\n",
    "    # merge years under NAICS code\n",
    "    temp1 = df90.merge(df00, how='outer', left_on='fips', right_on='fips', suffixes=['_90', '_00'])\n",
    "    temp2 = df10.merge(df20, how='outer', left_on='fips', right_on='fips', suffixes=['_10', '_20'])\n",
    "    df = temp1.merge(temp2, how='outer', left_on='fips', right_on='fips')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee1ed86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load new NAICS CSVs\n",
    "folder = sorted(os.listdir('my_naics5'))\n",
    "uniquecodes = pd.Series(folder[1:]).str[6:11].unique()\n",
    "path = 'my_naics5/naics_'\n",
    "\n",
    "# use big loop to update all previously processed NAICS files\n",
    "\n",
    "# capture the incomplete sets\n",
    "missing_years = []\n",
    "\n",
    "for code in uniquecodes:\n",
    "    try:\n",
    "        df90 = pd.read_csv(path + code + '_1990.csv')\n",
    "    except:\n",
    "        missing_years.append([code, '1990'])\n",
    "    try:\n",
    "        df00 = pd.read_csv(path + code + '_2000.csv')\n",
    "    except:\n",
    "        missing_years.append([code, '2000'])\n",
    "    try:    \n",
    "        df10 = pd.read_csv(path + code + '_2010.csv')\n",
    "    except:\n",
    "        missing_years.append([code, '2010'])\n",
    "    try:    \n",
    "        df20 = pd.read_csv(path + code + '_2020.csv')\n",
    "    except:\n",
    "        missing_years.append([code, '2020'])\n",
    "        \n",
    "    #df10.loc[df10['fips'].isna() & df10['area_fips'].notna(), 'fips'] = df10['area_fips']\n",
    "    #df20.loc[df20['fips'].isna() & df20['area_fips'].notna(), 'fips'] = df20['area_fips']\n",
    "    \n",
    "    # merge years under NAICS code\n",
    "    temp1 = df90.merge(df00, how='outer', left_on='fips', right_on='fips', suffixes=['_90', '_00'])\n",
    "    temp2 = df10.merge(df20, how='outer', left_on='fips', right_on='fips', suffixes=['_10', '_20'])\n",
    "    df = temp1.merge(temp2, how='outer', left_on='fips', right_on='fips')\n",
    "    \n",
    "    # fill in missing variables\n",
    "    cols = ['name_90', 'County Title_90', 'State_90', 'region_90', 'type_kurt20_90']\n",
    "    for col in cols:\n",
    "        df[col] = df[col].fillna(df[col[:-3] + '_20'])\n",
    "        df[col] = df[col].fillna(df[col[:-3] + '_10'])\n",
    "        df[col] = df[col].fillna(df[col[:-3] + '_00'])\n",
    "    \n",
    "    # clean column names\n",
    "    cols = ['name_90', 'County Title_90', 'State_90',\n",
    "            'region_90', 'type_kurt20_90'] + df.filter(regex='annual').columns.tolist()\n",
    "    df = df[cols]\n",
    "    df.columns = df.columns.str.replace('_90', '')\n",
    "    df = df.rename(columns={'annual_avg_emplvl': 'annual_avg_emplvl_90'})\n",
    "    \n",
    "    # replace nulls with zeroes \n",
    "    empl_cols = df.columns[-4:]\n",
    "    df[empl_cols] = df[empl_cols].fillna(0)\n",
    "\n",
    "    # rate of change function\n",
    "    def rate_chg(df, year1, year2, chg):\n",
    "        df[chg] = np.where((df[year1]== 0),\n",
    "                          ((df[year2] - df[year1]) / 1).round(4),\n",
    "                          ((df[year2] - df[year1]) / df[year1]).round(4))\n",
    "    \n",
    "    # define new column namer\n",
    "    namer = empl_cols.str.split('_')\n",
    "\n",
    "    # calculate rate of change\n",
    "    for x in range(0,3):\n",
    "        rate_chg(df, empl_cols[x], empl_cols[x+1], 'chg_' + namer[x][2] + '_' + namer[x][3] + '_' + namer[x+1][3])\n",
    "    \n",
    "    # calculate total rate of change column (1990-2020)\n",
    "    df['chg_emplvl_90_20'] = np.where((df['annual_avg_emplvl_90']==0),\n",
    "                                      ((df['annual_avg_emplvl_20'] - df['annual_avg_emplvl_90']) / 1).round(4),\n",
    "                                      ((df['annual_avg_emplvl_20'] - df['annual_avg_emplvl_90']) / df['annual_avg_emplvl_90']).round(4))\n",
    "    \n",
    "    # store industry name\n",
    "    name_probs = []\n",
    "    try:\n",
    "        industry_name = df['name'][df['name'].notna()][0]\n",
    "    except:\n",
    "        name_probs.append(code)\n",
    "    \n",
    "    # export file\n",
    "    # print(code, df[df['County Title'].notna()].shape)\n",
    "    df.to_csv('my_naics5_chg/naics_' + code + '_' + industry_name + '.csv',\n",
    "               index_label=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ppd599)",
   "language": "python",
   "name": "ppd599"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
