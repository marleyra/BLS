{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2a68aeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f6bb034c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['31', '32', '33', '51', '52', '54', '61', '62', '71'], dtype=object)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder = sorted(os.listdir('my_naics5_chg'))\n",
    "uniquecodes = pd.Series(folder[2:]).str[6:8].unique()\n",
    "uniquecodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f979fbba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "annual_avg_emplvl_90    245620.0\n",
       "annual_avg_emplvl_00    218265.0\n",
       "annual_avg_emplvl_10    139324.0\n",
       "annual_avg_emplvl_20     62879.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "umask = temp_df['type_kurt20'] == 'Metro'\n",
    "rmask = temp_df['type_kurt20'] == 'Rural'\n",
    "temp_df[umask][temp_df.filter(regex='annual').columns].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1439b2c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "code = []\n",
    "\n",
    "for file in folder:\n",
    "    if '51' in file[6:8]:\n",
    "        temp_df = pd.read_csv('my_naics5_chg/' + file)\n",
    "        umask = temp_df['type_kurt20'] == 'Metro'\n",
    "        rmask = temp_df['type_kurt20'] == 'Rural'\n",
    "        data.append(pd.concat([pd.Series(file[12:-4], index=['name']),\n",
    "                               temp_df[temp_df.filter(regex='annual').columns].sum(),\n",
    "                               temp_df[umask][temp_df.filter(regex='annual').columns].sum(),\n",
    "                               temp_df[rmask][temp_df.filter(regex='annual').columns].sum()]))\n",
    "        code.append(file[6:11])\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "df = pd.DataFrame(dict(zip(code, data))).T    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3d5f39b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in uniquecodes:\n",
    "    data = []\n",
    "    code = []\n",
    "    for file in folder:\n",
    "        if n in file[6:8]:\n",
    "            temp_df = pd.read_csv('my_naics5_chg/' + file)\n",
    "            umask = temp_df['type_kurt20'] == 'Metro'\n",
    "            rmask = temp_df['type_kurt20'] == 'Rural'\n",
    "            data.append(pd.concat([pd.Series(file[12:-4], index=['name']),\n",
    "                                   temp_df[temp_df.filter(regex='annual').columns].sum(),\n",
    "                                   temp_df[umask][temp_df.filter(regex='annual').columns].sum(),\n",
    "                                   temp_df[rmask][temp_df.filter(regex='annual').columns].sum()]))\n",
    "            code.append(file[6:11])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    pd.DataFrame(dict(zip(code, data))).T.to_csv('analysis/industry_naics_' + n + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c6bc13d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('analysis/industry_naics_' + '51' + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a2b61621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rate of change function\n",
    "def rate_chg(df, year1, year2, chg):\n",
    "    df[chg] = np.where((df[year1]== 0),\n",
    "                       (df[year2] - df[year1]) / 1,\n",
    "                       (df[year2] - df[year1]) / df[year1])\n",
    "    \n",
    "# define new column namer\n",
    "empl_cols = df.filter(regex='annual').columns\n",
    "namer = empl_cols.str.split('_')\n",
    "\n",
    "# calculate rate of change\n",
    "for x in range(0,3):\n",
    "    rate_chg(df,\n",
    "             empl_cols[x],\n",
    "             empl_cols[x+1],\n",
    "             'chg_' + namer[x][2] + '_' + namer[x][3] + '_' + namer[x+1][3])\n",
    "    \n",
    "# calculate total rate of change column (1990-2020)\n",
    "df['chg_emplvl_90_20'] = np.where((df['annual_avg_emplvl_90']==0),\n",
    "                                 ((df['annual_avg_emplvl_20'] - df['annual_avg_emplvl_90']) / 1),\n",
    "                                 ((df['annual_avg_emplvl_20'] - df['annual_avg_emplvl_90']) / df['annual_avg_emplvl_90']))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eba9ebcf",
   "metadata": {},
   "source": [
    "# create total jobs row\n",
    "df = pd.concat([df, pd.DataFrame(df[df.columns[1:]].sum(), columns=['51']).T])\n",
    "df.loc['51', 'name'] = 'Industry total'\n",
    "\n",
    "# wipe out total row's rate change values\n",
    "df.iloc[-1, -4:] = 0\n",
    "\n",
    "# repopulate w correct values\n",
    "empl_cols = df.filter(regex='annual').columns\n",
    "\n",
    "for x in range(0,3):\n",
    "    rate_chg(df.iloc[-1],\n",
    "             empl_cols[x],\n",
    "             empl_cols[x+1],\n",
    "             'chg_' + namer[x][2] + '_' + namer[x][3] + '_' + namer[x+1][3])\n",
    "    \n",
    "# calculate total rate of change column (1990-2020)\n",
    "df.loc['51', 'chg_emplvl_90_20'] = (df.loc['51', 'annual_avg_emplvl_20'] - df.loc['51', 'annual_avg_emplvl_90']) / df.loc['51', 'annual_avg_emplvl_90']\n",
    "df.to_csv('analysis/industry_naics_' + '51' + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1454df5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e6b2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataframes, calculates and stores totals by geography and region\n",
    "path = 'my_naics5_chg/naics_'\n",
    "codes = ['11', '21', '31', '51', '52', '54']\n",
    "\n",
    "# empty lists for processing\n",
    "shell = []\n",
    "names = []\n",
    "\n",
    "for code in codes:\n",
    "    df = pd.read_csv(path + code + '.csv')\n",
    "\n",
    "    classification = 'type_census20' # classification scheme\n",
    "    \n",
    "    temp_list = []\n",
    "    for col in df.columns[8:12]:\n",
    "        s = df.groupby(classification)[col].sum()\n",
    "        temp_list.append(s)\n",
    "    mynaics = pd.concat(temp_list, axis=1)\n",
    "    \n",
    "    # store values in dictionary\n",
    "    shell.append(mynaics)\n",
    "    names.append('geo_naics' + code)\n",
    "d1 = dict(zip(names, shell))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327ea233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate regional totals and save to dictionary\n",
    "\n",
    "# clear lists\n",
    "shell = []\n",
    "names = []\n",
    "\n",
    "for code in codes:\n",
    "    df = pd.read_csv(path + code + '.csv')\n",
    "    \n",
    "    temp_list = []\n",
    "    for col in df.columns[8:12]:\n",
    "        s = df.groupby('region')[col].sum()\n",
    "        temp_list.append(s)\n",
    "    mynaics = pd.concat(temp_list, axis=1)\n",
    "    \n",
    "    # store values in dictionary\n",
    "    shell.append(mynaics)\n",
    "    names.append('region_naics' + code)\n",
    "d2 = dict(zip(names, shell))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32231bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for code in codes:\n",
    "    print(d1['geo_naics' + code].sum() - d2['region_naics' + code].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1228bf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate state totals (preserving region) and save to dictionary\n",
    "\n",
    "# clear lists\n",
    "shell = []\n",
    "names = []\n",
    "\n",
    "for code in codes:\n",
    "    temp_list = []\n",
    "    for col in df.columns[8:12]:\n",
    "        s = df.groupby(['State', 'region'], as_index=False)[col].sum()\n",
    "        temp_list.append(s)\n",
    "    mynaics = pd.concat(temp_list, axis=1)\n",
    "    \n",
    "    # drop duplicate state and region cols\n",
    "    mynaics = mynaics.iloc[:,~mynaics.columns.duplicated()]\n",
    "    \n",
    "    # store values in dictionary\n",
    "    shell.append(mynaics)\n",
    "    names.append('state_naics' + code)\n",
    "d3 = dict(zip(names, shell))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b855d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create total jobs row\n",
    "dicts = [d1, d2]\n",
    "for d in dicts:\n",
    "    for key in d.keys():\n",
    "        d[key] = pd.concat([d[key],\n",
    "                            pd.DataFrame([d[key].sum()], index=['Total'], columns=d[key].columns),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99400fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create rate of change function\n",
    "def rate_chg(df, year1, year2, chg):\n",
    "    df[chg] = np.where((df[year1]==0),\n",
    "                       ((df[year2] - df[year1]) / 1).round(4),\n",
    "                       ((df[year2] - df[year1]) / df[year1]).round(4))\n",
    "    \n",
    "# create geographical and regional national job share and change columns\n",
    "dicts = [d1, d2]\n",
    "for d in dicts:\n",
    "    for key in d.keys():\n",
    "        # job share (removing total jobs row)\n",
    "        for col in d[key].columns:\n",
    "            d[key]['emplpct_' + col[-2:]] = (d[key].iloc[:-1][col] /\n",
    "                                             d[key].iloc[:-1][col].sum()).round(4)\n",
    "\n",
    "        # define % change column namer\n",
    "        namer = d[key].columns.str.split('_')\n",
    "        for x in range(0,3):\n",
    "            \n",
    "            # % change 90-00, 00-10, 10-20\n",
    "            rate_chg(d[key], d[key].columns[x], d[key].columns[x+1], 'pct_chg_' + namer[x][3] + '_' + namer[x+1][3])\n",
    "            \n",
    "        # overall % change 90-20\n",
    "        d[key]['pct_chg_90_20'] = np.where((d[key]['annual_avg_emplvl_90'] == 0),\n",
    "                                          ((d[key]['annual_avg_emplvl_20'] - d[key]['annual_avg_emplvl_90']) / 1).round(4),\n",
    "                                          ((d[key]['annual_avg_emplvl_20'] - d[key]['annual_avg_emplvl_90']) / d[key]['annual_avg_emplvl_90']).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105fdafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## issues with the pct change function when calculating regiona and geo totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c5a499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do same for states, make some tweaks for the different df structure\n",
    "for key in d3.keys():\n",
    "    # job share\n",
    "    for col in d3[key].columns[2:]:\n",
    "        d3[key]['emplpct_' + col[-2:]] = (d3[key].iloc[:-1][col] / \n",
    "                                          d3[key].iloc[:-1][col].sum()).round(4)\n",
    "\n",
    "    # define % change column namer\n",
    "    namer = d3[key].columns.str.split('_')\n",
    "    for x in range(0,3):\n",
    "            \n",
    "        # % change 90-00, 00-10, 10-20\n",
    "        rate_chg(d3[key], d3[key].columns[2:][x], d3[key].columns[2:][x+1], 'pct_chg_' + namer[x+2][3] + '_' + namer[x+3][3])\n",
    "            \n",
    "        # overall % change 90-20\n",
    "        d3[key]['pct_chg_90_20'] = np.where((d3[key]['annual_avg_emplvl_90'] == 0),\n",
    "                                           ((d3[key]['annual_avg_emplvl_20'] - d3[key]['annual_avg_emplvl_90']) / 1).round(4),\n",
    "                                           ((d3[key]['annual_avg_emplvl_20'] - d3[key]['annual_avg_emplvl_90']) / d3[key]['annual_avg_emplvl_90']).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2500f4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to CSVs\n",
    "for key in d1.keys():\n",
    "    d1[key].to_csv('analysis/geo_naics_' + key[-2:] + '.csv',) #index_label=False)\n",
    "\n",
    "for key in d2.keys():\n",
    "    d2[key].drop('Other').to_csv('analysis/region_naics_' + key[-2:] + '.csv',)# index_label=False)\n",
    "    \n",
    "for key in d3.keys():\n",
    "    d3[key].to_csv('analysis/state_naics_' + key[-2:] + '.csv',)# index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accb87d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ppd599)",
   "language": "python",
   "name": "ppd599"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
